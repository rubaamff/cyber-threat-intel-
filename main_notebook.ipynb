{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cyber Threat Intelligence Framework - Main Notebook\n",
        "\n",
        "This notebook demonstrates the complete pipeline of the Cyber Threat Intelligence Framework, including data preprocessing, feature engineering, model training, evaluation, and logging to a simulated blockchain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q gdown imbalanced-learn transformers scikit-learn tensorflow"
      ],
      "metadata": {
        "id": "install-packages"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import joblib\n",
        "\n",
        "# Import custom modules\n",
        "from preprocess_data import load_and_preprocess_data, feature_selection_and_scaling\n",
        "from extract_iocs import extract_iocs\n",
        "from train_classifiers import train_supervised_models, train_unsupervised_models, train_lstm_model, train_ensemble_model\n",
        "from blockchain_logger import log_predictions\n",
        "from evaluation_metrics import save_conf_matrix, save_class_report\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "blockchain_path = \"models/blockchain.json\"\n",
        "\n",
        "if not os.path.exists(blockchain_path):\n",
        "    with open(blockchain_path, \"w\") as f:\n",
        "        json.dump([], f)"
      ],
      "metadata": {
        "id": "import-modules"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "We will load and preprocess the NSL-KDD and CICIDS2017 datasets. Note: For demonstration, we will download these datasets. In a real scenario, you might have them locally or from a secure source."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NSL-KDD dataset\n",
        "!wget -q https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt -O /content/KDDTrain+.txt\n",
        "\n",
        "# Download CICIDS2017 dataset (subset for demonstration)\n",
        "# Note: The original notebook used gdown to download a folder from Google Drive.\n",
        "# For reproducibility and to avoid external dependencies, we will simulate the presence of a subset file.\n",
        "# In a real setup, you would replace this with your actual data loading mechanism.\n",
        "# For now, we will create a dummy cicids_subset.csv in sample_data.\n",
        "\n",
        "# Create dummy cicids_subset.csv for demonstration\n",
        "dummy_cicids_path = \"sample_data/cicids_subset.csv\"\n",
        "if not os.path.exists(dummy_cicids_path):\n",
        "    # Create a minimal dummy CSV for the script to run without errors\n",
        "    dummy_df = pd.DataFrame({\n",
        "        'Flow Bytes/s': np.random.rand(100) * 1000,\n",
        "        'Flow Packets/s': np.random.rand(100) * 100,\n",
        "        'Fwd Header Length': np.random.rand(100) * 50,\n",
        "        'Fwd Packets/s': np.random.rand(100) * 10,\n",
        "        'Flow IAT Mean': np.random.rand(100) * 10000,\n",
        "        'Flow IAT Max': np.random.rand(100) * 50000,\n",
        "        'Fwd IAT Total': np.random.rand(100) * 20000,\n",
        "        'Subflow Fwd Bytes': np.random.rand(100) * 500,\n",
        "        'Fwd IAT Std': np.random.rand(100) * 1000,\n",
        "        'Flow IAT Std': np.random.rand(100) * 2000,\n",
        "        'Idle Std': np.random.rand(100) * 5000,\n",
        "        'Idle Max': np.random.rand(100) * 10000,\n",
        "        'Fwd Header Length.1': np.random.rand(100) * 50,\n",
        "        'Flow Duration': np.random.rand(100) * 100000,\n",
        "        'Init_Win_bytes_forward': np.random.rand(100) * 1000,\n",
        "        'Bwd Packet Length Min': np.random.rand(100) * 10,\n",
        "        'Bwd Packet Length Max': np.random.rand(100) * 100,\n",
        "        'Bwd Packet Length Mean': np.random.rand(100) * 50,\n",
        "        'Label': np.random.choice(['BENIGN', 'ATTACK'], 100)\n",
        "    })\n",
        "    dummy_df.to_csv(dummy_cicids_path, index=False)

        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(dummy_cicids_path), exist_ok=True)"
      ],
      "metadata": {
        "id": "download-data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data using the custom module\n",
        "# Use the downloaded KDDTrain+.txt and the dummy cicids_subset.csv\n",
        "df = load_and_preprocess_data(\"/content/KDDTrain+.txt\", \"sample_data/cicids_subset.csv\")\n",
        "print(\"Merged DataFrame head:\")\n",
        "print(df.head())\n",
        "print(\"Class distribution after merge:\")\n",
        "print(df[\"label\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "load-preprocess"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection and Scaling\n",
        "\n",
        "We apply mutual information for feature selection and then scale the features using StandardScaler."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, scaler, top_features = feature_selection_and_scaling(df)\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Top features selected:\", top_features.tolist())"
      ],
      "metadata": {
        "id": "feature-selection-scaling"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "We train and evaluate various supervised, unsupervised, and deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_models = train_supervised_models(X_train, X_test, y_train, y_test, blockchain_path)"
      ],
      "metadata": {
        "id": "train-supervised"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsupervised Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_unsupervised_models(X_test, y_test, blockchain_path)"
      ],
      "metadata": {
        "id": "train-unsupervised"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_lstm_model(X_train, X_test, y_train, y_test, blockchain_path)"
      ],
      "metadata": {
        "id": "train-lstm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ensemble_model(X_train, X_test, y_train, y_test, supervised_models, blockchain_path)"
      ],
      "metadata": {
        "id": "train-ensemble"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blockchain Log Analysis\n",
        "\n",
        "Reviewing the logged predictions in the simulated blockchain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(blockchain_path, \"r\") as f:\n",
        "    chain = json.load(f)\n",
        "\n",
        "block_df = pd.DataFrame(chain)\n",
        "print(\"Blockchain sample entries:\")\n",
        "print(block_df.tail(10))\n",
        "print(f\"\\nTotal entries in blockchain: {len(block_df)}\")"
      ],
      "metadata": {
        "id": "blockchain-analysis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance Analysis\n",
        "\n",
        "Saving and displaying feature importance based on mutual information."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame({\n",
        "    'feature': top_features,\n",
        "    'importance': mutual_info_classif(df[top_features], df['label'])\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "feature_importance.to_csv(\"models/feature_importance.csv\", index=False)\n",
        "print(\"\\nFeature importance saved to models/feature_importance.csv\")\n",
        "print(feature_importance.head())"
      ],
      "metadata": {
        "id": "feature-importance"
      }
    }
  ]
}

